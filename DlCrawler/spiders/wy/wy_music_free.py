# -*- coding: utf-8 -*-
"""
æ¨¡æ¿åç§°
    * æ¨¡æ¿ç¼–å·: 0801
    * æ¨¡æ¿åç§°: ç½‘æ˜“äº‘éŸ³ä¹æ¦œå•å…è´¹æ¨¡æ¿
--------------------
åŠŸèƒ½ï¼šæå–æŒ‡å®šURLçš„çš„å¸–å­è¯¦æƒ…ä¿¡æ¯ï¼ˆæ‰¹æ¬¡å·/æ­Œå•ID/æ­Œå•æ ‡é¢˜/æ­Œæ›²ID/æ­Œæ›²æ ‡é¢˜/æ—¶é•¿/æ­Œæ‰‹/ä¸“è¾‘/æ­Œæ›²urlï¼‰
ä½¿ç”¨ï¼š
    * çˆ¬å–å‰éœ€åœ¨wy_music_free_config.pyä¸­è®¾ç½®æ¨èçš„é…ç½®å‚æ•°
    * start_urlséœ€ä»data/start_urls/wy_music_free_urls.csvæ–‡ä»¶è¯»å–;
        - éœ€æ‰‹åŠ¨æ·»åŠ urlè‡³csvæ–‡ä»¶
        ğŸ”´ç‰¹åˆ«æ³¨æ„: urlä¸­ä¸èƒ½å¸¦"#"; è‹¥å½•å…¥æ­Œå•URL,å¯èƒ½å‡ºç°æ•°æ®è·å–ä¸å…¨ç°è±¡
            æ¦œå•URL,ç¤ºä¾‹:https://music.163.com/discover/toplist?id=991319590
            æ­Œå•URL,ç¤ºä¾‹:https://music.163.com/playlist?id=991319590 
    * å¢åŠ å¹¶å‘æ•°é‡,å‡å°‘æˆ–ç¦ç”¨äººç±»æ¨¡æ‹Ÿè¡Œä¸ºå¯ä»¥æå‡æŠ“å–æ•ˆç‡,ä½†ä¼šå¢åŠ åçˆ¬è™«é£é™©,éœ€æ ¹æ®å®é™…æƒ…å†µè‡ªè¡Œè°ƒæ•´
        - èµ·å§‹urlsä¸º1ä¸ªæ—¶,æœ‰æ•ˆå¹¶å‘æ•°é‡å§‹ç»ˆä¸º1
    * æœ¬æ¨¡æ¿æ˜¯å¦ç™»å½•å‡å¯,ä½†æ¨èç™»å½•
"""
import scrapy
from datetime import datetime
from DlCrawler.items import WyMusicFreeItem
from DlCrawler.configs.wy.wy_music_free_config import CUSTOM_SETTINGS
import random
import pathlib
import datetime
import re
import requests
import time

class WyMusicFreeSpider(scrapy.Spider):
    # åŸºç¡€å‚æ•°
    name = "wy_music_free"
    allowed_domains = ["music.163.com"]
    start_urls = []
    current_page = 1  # å½“å‰é¡µç 
    
    # åŠ è½½é…ç½®å‚æ•°
    custom_settings = CUSTOM_SETTINGS

    # æœ€å¤§è·å–æ•°é‡
    max_count = CUSTOM_SETTINGS['MAXCOUNT']
    success_count = 0
    
    # cookiesæ–‡ä»¶è·¯å¾„åˆå§‹åŒ–
    cookies_dir = pathlib.Path(__file__).parent.parent.parent / "data" / "cookies"
    cookies_dir.mkdir(parents=True, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºç›®å½•
    cookies_file = cookies_dir / f"{name}_cookies.txt"

    # å…¶å®ƒå‚æ•°
    retries = 0
    now = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    template_no = CUSTOM_SETTINGS['TEMPLATE_NO'] # æ¨¡æ¿ç¼–å·
    is_download_music = CUSTOM_SETTINGS['IS_DOWNLOAD_MUSIC']

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # åˆå§‹åŒ–è¯»å–URLåˆ—è¡¨
        self.start_urls = self.read_start_urls() or []
        
        # æ·»åŠ æ—¥å¿—è¾“å‡º
        self.logger.info(f"æˆåŠŸåŠ è½½ {len(self.start_urls)} ä¸ªèµ·å§‹URL")
        if self.start_urls:
            self.logger.debug(f"å‰3ä¸ªURL: {self.start_urls[:3]}")

    def read_start_urls(self):
        try:
            # è¯»å–urlsåˆ—è¡¨
            start_urls_path = pathlib.Path(__file__).parent.parent.parent / "data" / "start_urls" / f"{self.name}_urls.csv"  
            with open(start_urls_path, "r", encoding="utf-8") as f:
                lines = f.readlines()[1:]
                start_urls = [line.strip() for line in lines if line.strip()]
                
            # è¿‡æ»¤æ‰ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
            valid_urls = [url for url in start_urls if url and not url.startswith("#")]
            return valid_urls
        except Exception as e:
            self.logger.error(f"è¯»å–urlsåˆ—è¡¨å¤±è´¥ï¼š{e}")
            return None
    def parse_cookies(self, cookies_file):
        try:
            with open(cookies_file, "r") as f:
                raw_cookies = f.read().strip()
            cookies_dict = {}
            for item in raw_cookies.split(";"):
                item = item.strip()
                key, value = item.split("=", 1)
                cookies_dict[key] = value
            return cookies_dict
        except Exception as e:
            self.logger.error(f"è§£æcookiesæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return None
    def start_requests(self):
        # å‘èµ·ç™»å½•è¯·æ±‚
        for target_url in self.start_urls:
            if self.cookies_file.exists(): 
                cookies = self.parse_cookies(self.cookies_file)
                yield scrapy.Request(
                target_url,
                cookies=cookies,
                callback=self.parse
            )
    def parse(self, response):
        for item in self.parse_page_content(response):
            yield item
    def parse_page_content(self, response):
        # åˆå§‹åŒ–å¿…è¦å¯¹è±¡
        html = response.text

        # # ä¸´æ—¶è°ƒè¯•,éœ€è¦æ—¶è‡ªè¡Œå–æ¶ˆæ³¨é‡Š
        # try:
        #     debug_dir = pathlib.Path(__file__).parent.parent.parent / "debug_files"
        #     debug_dir.mkdir(parents=True, exist_ok=True)
        #     with open(f"{debug_dir}/{self.name}_p{self.current_page}.html", "w", encoding="utf-8") as f:
        #         f.write(html)
        # except Exception as e:
        #     self.logger.info(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{e}")

        playlist_id = response.url.split('=')[-1]

        title_line = response.xpath('//title').get()
        pattern = r'<title>(.*?) - .*?</title>'
        match = re.search(pattern, title_line)
        playlist_title = match.group(1).strip() if match else "Unknown Playlist"
       
        song_infos = re.findall(r'<a href="/song\?id=(\d+)">(.*?)</a>', html)

        for song_id,song_title in song_infos:  # éå†æ¯é¦–æ­Œæ›²ä¿¡æ¯
            if self.success_count >= self.max_count:
                self.logger.info(f"è¾¾åˆ°ä¸Šé™{self.max_count}, ç»ˆæ­¢å½“å‰é¡µè§£æ")
                break 
            item = WyMusicFreeItem()
            
            item['batch_id'] = self.now
            item['playlist_id'] = playlist_id
            item['playlist_title'] = playlist_title
            item['song_id'] = song_id
            item['song_title'] = song_title
            item['song_url'] = f"http://music.163.com/song/media/outer/url?id={song_id}.mp3"

            self.success_count += 1
           
            self.logger.info(f'''å·²è·å– ç¬¬{self.success_count} æ¡æ•°æ®:
                             playlist_id:{item['playlist_id']}
                             song_id:{item['song_id']}
                             song_title:{item['song_title']}
                             ...
''')
            
            if self.is_download_music:
                self.download_music(item['song_url'], item['song_title'],self.cookies_file)
            yield item

    def download_music(self, song_url, song_title,cookies_file):
        download_dir = pathlib.Path(__file__).parent.parent.parent / "download" / self.name
        download_dir.mkdir(parents=True, exist_ok=True)
        file_path = download_dir / f"{song_title}.mp3"
        with open(cookies_file, "r") as f:
            raw_cookies = f.read().strip()

        response = requests.get(song_url,headers={
                                'User-Agent': self.custom_settings['USER_AGENT'],
                                'cookies': raw_cookies,
                                'Referer': 'https://music.163.com/'
                                },
                                stream=True)
        if response.status_code == 200:
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
            self.logger.info(f"âœ…æˆåŠŸä¸‹è½½éŸ³ä¹: {song_title}.mp3")
        else:
            self.logger.error(f"âŒä¸‹è½½å¤±è´¥: {song_title} (çŠ¶æ€ç : {response.status_code})")

        time.sleep(random.uniform(0.5, 1.5))
        print(f"ğŸ•‘éšæœºä¼‘çœ 0.5-1.5ç§’")